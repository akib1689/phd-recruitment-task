{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Most Vulnerable Projects - Data analysis\n",
    "\n",
    "This notebook aims to find the most vulnerable projects based on control flow graph (CFG) metrics extracted from code changes. It processes a dataset of code changes, dot-formatted CFGs, and computes various metrics to identify projects with potentially risky code changes.\n",
    "\n",
    "This Notebook answers these questions:\n",
    "\n",
    "1. From the selected projects, what are the top 10 that have the most commits on fixing bugs, issues, and vulnerabilities?\n",
    "\n",
    "    1. What are their common characteristics? For example, are they all related to machine learning, to mobile development, etc?\n",
    "\n",
    "    1. Is there anything that all the top projects don't have in common?\n",
    "\n",
    "    1. Do any of those projects have incivil commits? By incivil comments, we mean commit messages containing offensive, rude, or hostile language (e.g., as detected by a toxicity classifier or manual review).\n",
    "\n",
    "    1. Regarding the CFGs of those projects, is the depth of the CFG correlated in any way to the existence of bugs, vulnerabilities, and issues?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from IPython.display import display, SVG\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.ParserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "# Ensure the `utils` module is on the Python path\n",
    "sys.path.insert(0, os.path.abspath('utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../assets/data-samples/\"\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in sorted(csv_files):\n",
    "    size_mb = os.path.getsize(os.path.join(data_dir, f)) / (1024**2)\n",
    "    print(f\"  - {f} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files into a single DataFrame\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No CSV files found in directory: {data_dir}\")\n",
    "sample_file = os.path.join(data_dir, csv_files[-1])\n",
    "# Read the CSV with low_memory=False to avoid DtypeWarning from mixed types across chunks\n",
    "# and explicitly set common text columns to string dtype to ensure consistency\n",
    "df = pd.read_csv(sample_file, dtype={\n",
    "  'project_name': 'string',\n",
    "  'project_description': 'string',\n",
    "  'project_url': 'string',\n",
    "  'project_creation_date': 'string',\n",
    "  'project_database': 'string',\n",
    "  'project_languages': 'string',\n",
    "  'project_oss': 'string',\n",
    "  'project_topics': 'string',\n",
    "  'commit_url': 'string',\n",
    "  'file_path': 'string',\n",
    "  'method_name': 'string',\n",
    "  'cfg_dot': 'string',\n",
    "  'cfg_state': 'string'\n",
    "})\n",
    "\n",
    "initial_len = len(df)\n",
    "print(f\"Loaded {initial_len} CFG entries from {os.path.basename(sample_file)}\")\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.is_column_empty import is_effectively_empty\n",
    "\n",
    "cols_to_drop = [col for col in df.columns if is_effectively_empty(df[col])]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"Removing effectively empty columns: {cols_to_drop}\")\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "print(f\"DataFrame now has {df.shape[1]} columns after removing empty ones.\")\n",
    "# print the datatyps of the remaining columns\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- make the project language column more usable ---\n",
    "from utils.parse_language_series import split_semicolon_series\n",
    "df['project_languages'] = split_semicolon_series(df['project_languages'])\n",
    "\n",
    "print(f\"Unique programming languages found: {df['project_languages'].explode().nunique()}\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Top 10 projects with most commits fixing bugs, issues, and vulnerabilities\n",
    "\n",
    "**Question:** From the selected projects, what are the top 10 that have the most commits on fixing bugs, issues, and vulnerabilities?\n",
    "\n",
    "To answer this question, we will take this approach:\n",
    "\n",
    "- Remove the rows with `PRE` in the `cfg_state` column, as they do not represent actual code changes.\n",
    "- Group the DataFrame by `project_name` and count the number of commits for each project.\n",
    "- Sort the projects by the number of commits in descending order and select the top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Remove \"PRE\" rows (non-change states) and count unique commits per project\n",
    "df_changes = df[~df['cfg_state'].str.upper().eq('PRE')].copy()\n",
    "removed_count = len(df) - len(df_changes)\n",
    "print(f\"Removed {removed_count} PRE rows -> {len(df_changes)} rows remaining (of {len(df)})\")\n",
    "\n",
    "# Unique commit count per project (a commit may touch multiple files/methods; count unique commit_url)\n",
    "commit_counts = df_changes.groupby('project_name')['commit_url'].nunique().reset_index(name='num_commits')\n",
    "\n",
    "# Additional useful aggregations for context:\n",
    "agg = df_changes.groupby('project_name').agg(\n",
    "  project_description=('project_description', lambda s: s.dropna().iloc[0] if s.notna().any() else pd.NA),\n",
    "  project_languages=('project_languages', 'first'),\n",
    "  project_url=('project_url', 'first'),\n",
    "  total_files_changed=('files_changed_count', 'sum'),\n",
    "  unique_files_changed=('file_path', 'nunique'),\n",
    "  unique_methods_changed=('method_name', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Merge commit counts with aggregated metadata and sort\n",
    "project_stats = commit_counts.merge(agg, on='project_name')\n",
    "project_stats = project_stats.sort_values('num_commits', ascending=False)\n",
    "\n",
    "# Top 10 projects by number of unique commits fixing issues/bugs/vulns\n",
    "top_n = 10\n",
    "top_projects = project_stats.head(top_n).reset_index(drop=True)\n",
    "\n",
    "# Add percent of total unique commits to give context\n",
    "total_unique_commits = commit_counts['num_commits'].sum()\n",
    "top_projects['pct_of_total_commits'] = (top_projects['num_commits'] / total_unique_commits * 100).round(2)\n",
    "\n",
    "display(top_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "**Potential Improvements:**\n",
    "The last column `pct_of_total_commits` could be combined with full context. For example the above figure we can see that [ceylon/ceylon-compiler](https://github.com/ceylon/ceylon-compiler ) accounts for **36%** of the total commits fixing bugs, issues, and vulnerabilities. However, without knowing the total number of commits in a project, this percentage alone does not provide a complete picture of the project's overall activity or the significance of bug-fixing commits relative to its total development efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Characteristics of Top 10 Projects\n",
    "\n",
    "**Question:** What are their common characteristics? For example, are they all related to machine learning, to mobile development, etc?\n",
    "\n",
    "**Question:** Is there anything that all the top projects don't have in common?\n",
    "\n",
    "\n",
    "As we can see from the table, all the top 10 projects are: \n",
    "\n",
    "#### Top 10 Projects\n",
    "\n",
    "| Rank | Project Name | Num Commits | Description | Primary Languages |\n",
    "|------|--------------|-------------|-------------|-------------------|\n",
    "| 1 | `ceylon/ceylon-compiler` | 2126 | Ceylon compiler (Java backend) | Perl, Ruby, Shell, C, Java... |\n",
    "| 2 | `aokpx-private/platform_packages_apps_Calendar` | 1040 | (Android Calendar App) | Java |\n",
    "| 3 | `dana-i2cat/opennaas-routing-nfv` | 679 | OpenNaaS Routing NFV | Java, CSS, JS, Shell |\n",
    "| 4 | `rfkrocktk/red5-server` | 630 | Live Video Streaming Server | Java, Shell, JS |\n",
    "| 5 | `eclipse/webtools.jsf` | 595 | Eclipse Web Tools Platform | Java, CSS |\n",
    "| 6 | `ebayopensource/turmeric-runtime` | 66 | Turmeric SOA Runtime | Java, Shell, Perl |\n",
    "| 7 | `mibto/mez` | 58 | Zeiterfassung Metzler (Time Tracking) | Java |\n",
    "| 8 | `Ourobor/petulant-batman` | 49 | Group Project for an SE Class | Java |\n",
    "| 9 | `venukumar/bartsy-venue-android` | 47 | (Android App) | Java |\n",
    "| 10 | `ovitas/compass2` | 30 | Compass 2 for Sesam 4 | Java, JS |\n",
    "\n",
    "---\n",
    "\n",
    "### Similarities\n",
    "\n",
    "1.  **Dominance of Java**:\n",
    "    *   **All 10 projects** utilize **Java** as a primary or significant language. This indicates a strong homogeneity in the underlying technology stack of the most \"fix-heavy\" projects in this dataset. It suggests the control flow graph analysis or the dataset collection process might have been heavily focused on Java repositories.\n",
    "    \n",
    "1.  **Infrastructure & Tooling Orientation**:\n",
    "    -   A significant portion of the top projects are **developer tools, frameworks, or infrastructure** rather than simple end-user applications.\n",
    "        -   *Compiler*: `ceylon-compiler`\n",
    "        -   *Server*: `red5-server`\n",
    "        -   *Dev Tools*: `webtools.jsf`\n",
    "        -   *Framework*: `turmeric-runtime`, `opennaas-routing-nfv`   \n",
    "\n",
    "This suggests that complex infrastructure projects might require more frequent bug fixing or have more rigorous commit practices.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Dissimilarities\n",
    "\n",
    "1.  **Scale of Activity (The \"Power Law\" Distribution)**:\n",
    "    -   There is a **massive disparity** in fix volume between the top and the bottom of this list.\n",
    "    -   The top project (`ceylon-compiler`) has **2,126** fix commits.\n",
    "    -   The 6th project (`turmeric-runtime`) drops sharply to **66** commits.\n",
    "    -   The 10th project (`compass2`) has only **30** commits.\n",
    "    -   This indicates that the dataset is dominated by a handful of extremely active projects, while the \"long tail\" consists of much smaller or less active repositories.\n",
    "\n",
    "1.  **Project Governance & Maturity**:\n",
    "    -   **Corporate/Foundation**: Projects like `eclipse/webtools.jsf` (Eclipse Foundation) and `ebayopensource/turmeric-runtime` (eBay) likely follow strict corporate or foundation governance models.\n",
    "    -   **Community/Indie**: Projects like `rfkrocktk/red5-server` (Red5) are community-driven open source.\n",
    "    -   **Student/Academic**: `Ourobor/petulant-batman` is explicitly described as a \"Group Project for an SE Class\", representing a vastly different level of maturity and code quality standards compared to Eclipse or eBay.\n",
    "\n",
    "1.  **Application Domain**:\n",
    "    -   The functional domains are quite distinct:\n",
    "        -   **Language Engineering**: `ceylon-compiler`\n",
    "        -   **Mobile/Android**: `platform_packages_apps_Calendar`, `bartsy-venue-android`\n",
    "        -   **Media Streaming**: `red5-server`\n",
    "        -   **Networking/NFV**: `opennaas-routing-nfv`\n",
    "        -   **Enterprise SOA**: `turmeric-runtime`\n",
    "\n",
    "This diversity shows that high bug-fix counts are not unique to one specific industry vertical.\n",
    "\n",
    "\n",
    "While the top 10 projects share a common technical foundation, they represent a highly diverse set of governance models, domains, and scales. The presence of both enterprise-grade foundations (Eclipse) and student projects (petulant-batman) in the top 10 highlights the varied nature of the dataset. The extreme skew in commit counts (2000+ vs <70) suggests that any aggregate analysis should be careful not to be overwhelmed by the top 1-5 dominant projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Incivil Commits in Top Projects\n",
    "\n",
    "**Question:** Do any of those projects have incivil commits? By incivil comments, we mean commit messages containing offensive, rude, or hostile language (e.g., as detected by a toxicity classifier or manual review).\n",
    "\n",
    "To determine if any of the top projects have incivil commits, we can analyze the commit messages associated with each commit in the dataset. I used a pre-trained toxicity classifier (e.g., from Hugging Face Transformers `unitary/toxic-bert` model) to identify potentially incivil language in commit messages.\n",
    "\n",
    "First, we load the toxicity classifier and define a function to classify commit messages\n",
    "\n",
    "The settings that I used for the toxicity classifier are as follows:\n",
    "\n",
    "- **Model:** `unitary/toxic-bert`\n",
    "- **Threshold:** 0.7 (A commit message is considered incivil if the toxicity score exceeds this threshold)\n",
    "- **Device:** MPS (Apple Silicon) if available, otherwise CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "#  Pre-check MPS — safer and more explicit\n",
    "device = 0 if torch.backends.mps.is_available() else -1\n",
    "print(f\"Using device: {'MPS' if device == 0 else 'CPU'}\")\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"unitary/toxic-bert\",\n",
    "    device=device,\n",
    "    top_k=None,\n",
    "    truncation=True,          # ← critical: avoids errors on long commit messages\n",
    "    batch_size=8,             # ← enables batching for faster apply()\n",
    ")\n",
    "\n",
    "def is_toxic(text, threshold=0.7):\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return False\n",
    "        # classifier returns list of list of dicts: [[{label, score}, ...], ...]\n",
    "        preds = classifier(text)[0]  # list of label-score dicts for one input\n",
    "        toxic_score = next((r['score'] for r in preds if r['label'] == 'toxic'), 0.0)\n",
    "        return toxic_score > threshold\n",
    "    except Exception as e:\n",
    "        # Optional: log problematic entries (e.g., encoding issues)\n",
    "        # print(f\"Skipped text (len={len(text)}): {e}\")\n",
    "        return False  # conservative: treat failures as non-toxic\n",
    "    \n",
    "top_project_names = set(top_projects['project_name'])\n",
    "df_top_projects = df[df['project_name'].isin(top_project_names)].copy()\n",
    "df_top10_unique_commits = df_top_projects.drop_duplicates(subset=['commit_url']).copy()\n",
    "\n",
    "print(f\"Filtered to {len(df_top_projects)} rows ({len(df_top10_unique_commits)} unique commits) from top {len(top_project_names)} projects\")\n",
    "\n",
    "# Now apply toxicity classifier just once per unique commit (more efficient + avoids redundancy)\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "df_top10_unique_commits['is_toxic'] = df_top10_unique_commits['commit_message'].progress_apply(is_toxic)\n",
    "\n",
    "# print top five toxic commits for inspection\n",
    "display(df_top10_unique_commits[df_top10_unique_commits['is_toxic']].head(5)[['project_name', 'commit_url', 'commit_message']])\n",
    "\n",
    "toxic_counts = df_top10_unique_commits.groupby('project_name')['is_toxic'].agg(\n",
    "    toxic_commit_count='sum',\n",
    "    total_commits='size'  # same as nunique(commit_url) here, since deduped\n",
    ").reset_index()\n",
    "\n",
    "top_projects_enhanced = top_projects.merge(toxic_counts, on='project_name')\n",
    "top_projects_enhanced['toxic_ratio'] = (\n",
    "    top_projects_enhanced['toxic_commit_count'] / top_projects_enhanced['num_commits']\n",
    ").round(4)\n",
    "\n",
    "\n",
    "display(top_projects_enhanced[['project_name', 'num_commits', 'toxic_commit_count', 'toxic_ratio']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "**Decision:**\n",
    "There are no incivil commits in the top 10 projects based on the toxicity analysis of commit messages using the `unitary/toxic-bert` model.\n",
    "\n",
    "**Potential Improvements:**\n",
    "- Consider using multiple toxicity detection models to cross-validate results and improve accuracy.\n",
    "- Manually review a sample of commit messages flagged as incivil to ensure the model's accuracy and relevance to the context of software development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Correlation Between CFG Depth and Bugs/Vulnerabilities\n",
    "\n",
    "**Question:** Regarding the CFGs of those projects, is the depth of the CFG correlated in any way to the existence of bugs, vulnerabilities, and issues?\n",
    "\n",
    "To analyze the correlation between CFG depth and the existence of bugs, vulnerabilities, and issues, we can follow these steps:\n",
    "\n",
    "- We’ll treat a method change unit as a unique (`project_name`, `commit_url`, `file_path`, `method_name`) that has exactly one `pre` and one `post` entry.\n",
    "- Compute Depth (and other metrics) for Both CFGs\n",
    "- Associate the matrices with keywords like bug, vulnerability, issue from commit messages\n",
    "- Now you can ask nuanced questions:\n",
    "\n",
    "    - Do security fixes reduce depth more than generic bug fixes?\n",
    "    - Do refactors reduce depth the most?\n",
    "    - Do features increase depth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.info())\n",
    "\n",
    "df_clean = df[\n",
    "    df['cfg_state'].isin(['PRE', 'POST']) &\n",
    "    df['cfg_dot'].notna()\n",
    "].copy()\n",
    "\n",
    "df_clean['method_key'] = (\n",
    "    df_clean['project_name'] + '|' +\n",
    "    df_clean['commit_url'] + '|' +\n",
    "    df_clean['file_path'] + '|' +\n",
    "    df_clean['method_name']\n",
    ")\n",
    "\n",
    "state_counts = df_clean.groupby('method_key')['cfg_state'].value_counts().unstack(fill_value=0)\n",
    "\n",
    "\n",
    "paired_keys = state_counts[(state_counts['PRE'] == 1) & (state_counts['POST'] == 1)].index\n",
    "paired_df = df_clean[df_clean['method_key'].isin(paired_keys)].copy()\n",
    "\n",
    "# Statistics on paired methods\n",
    "num_paired_methods = len(paired_keys)\n",
    "num_unique_projects = paired_df['project_name'].nunique()\n",
    "print(f\"Found {num_paired_methods} paired methods across {num_unique_projects} unique projects.\")\n",
    "\n",
    "print(paired_df.info())\n",
    "print(f\"Found {len(paired_df)} total rows in paired DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we pivot the table\n",
    "\n",
    "wide = paired_df.pivot(\n",
    "    index='method_key',\n",
    "    columns='cfg_state',\n",
    "    values=['cfg_dot', 'project_name', 'commit_url', 'file_path', 'method_name', 'commit_message']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten MultiIndex columns\n",
    "wide.columns = [\n",
    "    f\"{col[0]}_{col[1]}\" if col[1] else col[0]\n",
    "    for col in wide.columns\n",
    "]\n",
    "\n",
    "# Final tidy dataset\n",
    "changes = wide[[\n",
    "    'project_name_PRE', 'commit_url_PRE', 'file_path_PRE', 'method_name_PRE',\n",
    "    'commit_message_PRE', 'cfg_dot_PRE', 'cfg_dot_POST'\n",
    "]].rename(columns={\n",
    "    'project_name_PRE': 'project_name',\n",
    "    'commit_url_PRE': 'commit_url',\n",
    "    'file_path_PRE': 'file_path',\n",
    "    'method_name_PRE': 'method_name',\n",
    "    'commit_message_PRE': 'commit_message',\n",
    "    'cfg_dot_PRE': 'cfg_pre',\n",
    "    'cfg_dot_POST': 'cfg_post'\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "print(f\"Final changes DataFrame has {len(changes)} rows.\")\n",
    "print(changes.info())\n",
    "display(changes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.cfg_utils import dot_to_graph, get_depth, count_nodes_edges\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "changes['G_pre'] = changes['cfg_pre'].progress_apply(dot_to_graph)\n",
    "changes['G_post'] = changes['cfg_post'].progress_apply(dot_to_graph)\n",
    "\n",
    "\n",
    "changes['nodes_pre'], changes['edges_pre'] = zip(*changes['G_pre'].apply(count_nodes_edges))\n",
    "changes['depth_pre'] = changes['G_pre'].apply(get_depth)\n",
    "\n",
    "changes['nodes_post'], changes['edges_post'] = zip(*changes['G_post'].apply(count_nodes_edges))\n",
    "changes['depth_post'] = changes['G_post'].apply(get_depth)\n",
    "\n",
    "changes['delta_depth'] = changes['depth_post'] - changes['depth_pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure commit_message is string (handle NaNs)\n",
    "changes['commit_message'] = changes['commit_message'].fillna(\"\").astype(str)\n",
    "\n",
    "from utils.classifier import classify_fix_type\n",
    "\n",
    "# Apply\n",
    "changes['fix_type'] = changes['commit_message'].apply(classify_fix_type)\n",
    "changes['is_fix'] = changes['fix_type'] != \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check\n",
    "print(\"Fix type distribution:\")\n",
    "print(changes['fix_type'].value_counts().sort_index())\n",
    "\n",
    "# Example: top 5 security-fixing commits with largest depth reduction\n",
    "(\n",
    "    changes[changes['fix_type'] == 'security']\n",
    "    .sort_values('delta_depth')\n",
    "    .head(5)[['project_name', 'file_path', 'method_name', 'commit_message', 'depth_pre', 'depth_post', 'delta_depth']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Depth = 0:\", (changes['depth_pre'] == 0).sum(), \"/\", len(changes))\n",
    "print(\"Depth = 1:\", (changes['depth_pre'] == 1).sum())\n",
    "print(\"Depth ≥ 5:\", (changes['depth_pre'] >= 5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define buckets\n",
    "def depth_bucket(d):\n",
    "    if d <= 2:\n",
    "        return 'shallow (≤2)'\n",
    "    elif d <= 5:\n",
    "        return 'medium (3–5)'\n",
    "    elif d <= 10:\n",
    "        return 'moderate (6–10)'\n",
    "    elif d <= 20:\n",
    "        return 'deep (11–20)'\n",
    "    else:\n",
    "        return 'very_deep (>20)'\n",
    "\n",
    "changes['depth_bucket'] = changes['depth_pre'].apply(depth_bucket)\n",
    "\n",
    "# Analyze by bucket + fix_type\n",
    "bucket_analysis = (\n",
    "    changes\n",
    "    .groupby(['depth_bucket', 'fix_type'])\n",
    "    .agg(\n",
    "        count=('delta_depth', 'size'),\n",
    "        delta_depth_mean=('delta_depth', 'mean'),\n",
    "        delta_depth_median=('delta_depth', 'median'),\n",
    "        pct_decreased=('delta_depth', lambda x: (x < 0).mean() * 100)\n",
    "    )\n",
    "    .round(3)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Pivot for readability\n",
    "pivot = bucket_analysis.pivot(\n",
    "    index='depth_bucket',\n",
    "    columns='fix_type',\n",
    "    values='delta_depth_mean'\n",
    ").round(2)\n",
    "\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Helper: robust mode (returns scalar)\n",
    "def mode_func(x):\n",
    "    try:\n",
    "        m = stats.mode(x.dropna(), keepdims=False)\n",
    "        return int(m.mode) if m.count > 0 else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Define aggregation specs for ONE column (reusable)\n",
    "def get_agg_spec(col_name: str):\n",
    "    return {\n",
    "        f'{col_name}_count': (col_name, 'size'),\n",
    "        f'{col_name}_mean': (col_name, 'mean'),\n",
    "        f'{col_name}_median': (col_name, 'median'),\n",
    "        f'{col_name}_mode': (col_name, mode_func),\n",
    "        f'{col_name}_std': (col_name, 'std'),\n",
    "        f'{col_name}_min': (col_name, 'min'),\n",
    "        f'{col_name}_max': (col_name, 'max'),\n",
    "        f'{col_name}_q25': (col_name, lambda x: np.percentile(x, 25)),\n",
    "        f'{col_name}_q75': (col_name, lambda x: np.percentile(x, 75)),\n",
    "        f'{col_name}_skew': (col_name, lambda x: stats.skew(x, nan_policy='omit')),\n",
    "        f'{col_name}_kurtosis': (col_name, lambda x: stats.kurtosis(x, nan_policy='omit')),\n",
    "        f'{col_name}_pct_ge_5': (col_name, lambda x: (x >= 5).mean() * 100),\n",
    "        f'{col_name}_pct_ge_10': (col_name, lambda x: (x >= 10).mean() * 100),\n",
    "        f'{col_name}_pct_ge_15': (col_name, lambda x: (x >= 15).mean() * 100),\n",
    "        f'{col_name}_pct_ge_20': (col_name, lambda x: (x >= 20).mean() * 100),\n",
    "    }\n",
    "\n",
    "# Apply separately to depth_pre and depth_post\n",
    "pre_agg = changes.groupby('fix_type').agg(**get_agg_spec('depth_pre'))\n",
    "post_agg = changes.groupby('fix_type').agg(**get_agg_spec('depth_post'))\n",
    "\n",
    "# Merge\n",
    "summary = pd.concat([pre_agg, post_agg], axis=1)\n",
    "\n",
    "# Now add delta stats (safe way)\n",
    "delta_stats = changes.groupby('fix_type').agg(\n",
    "    delta_mean=('delta_depth', 'mean'),\n",
    "    delta_median=('delta_depth', 'median'),\n",
    "    delta_std=('delta_depth', 'std'),\n",
    "    pct_reduced=('delta_depth', lambda x: (x < 0).mean() * 100),\n",
    "    pct_increased=('delta_depth', lambda x: (x > 0).mean() * 100),\n",
    "    pct_unchanged=('delta_depth', lambda x: (x == 0).mean() * 100),\n",
    ")\n",
    "\n",
    "summary = summary.join(delta_stats).round(2).reset_index()\n",
    "\n",
    "# Optional: reorder columns for readability\n",
    "priority_cols = [\n",
    "    'fix_type',\n",
    "    'depth_pre_mean', 'depth_post_mean',\n",
    "    'depth_pre_median', 'depth_post_median',\n",
    "    'depth_pre_mode', 'depth_post_mode',\n",
    "    'depth_pre_max', 'depth_post_max',\n",
    "    'depth_pre_pct_ge_10', 'depth_post_pct_ge_10',\n",
    "    'depth_pre_pct_ge_20', 'depth_post_pct_ge_20',\n",
    "    'delta_mean', 'delta_median', 'pct_reduced', 'pct_increased'\n",
    "]\n",
    "\n",
    "# Keep only columns that exist (in case some agg failed)\n",
    "existing_cols = [c for c in priority_cols if c in summary.columns]\n",
    "summary = summary[existing_cols]\n",
    "\n",
    "print(summary.to_string(index=False, float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "| Metric | Key Finding |\n",
    "|--------|-------------|\n",
    "| Pre-fix depth | Security and crash-related fixes occur in significantly deeper methods: <br> - security: median = 5, mean = 8.62 <br> - crash: median = 6, mean = 10.74 <br> - bug: median = 3, mean = 6.29 <br> - Vulnerabilities and crashes concentrate in more complex control flows. |\n",
    "| Depth change (Δ) | Mean Δdepth ≈ 0 for all categories, but direction matters: <br> - security: -0.01 (only category with negative mean) <br> - pct_reduced > pct_increased for security (1.00% vs 1.40%) — slight bias toward simplification <br> - crash: Δ = 0.00, but pct_reduced = 1.96% < pct_increased = 3.67% → net increase |\n",
    "| Tail complexity (≥20) | Security fixes: 9.01% → 8.78% (−0.23 pp) <br> Crash fixes: 14.11% → 14.19% (+0.08 pp) <br> → Only security fixes consistently reduce extreme complexity. |\n",
    "| Max depth unchanged | depth_pre_max == depth_post_max for all (e.g., security: 123→123) <br> - Top outliers are not being simplified — possibly due to architectural constraints or incomplete refactorings. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- Confirmation of complexity-risk hypothesis:\n",
    "Methods involved in security and crash fixes are 30–80% deeper (by median) than generic bug fixes — supporting the idea that control flow complexity contributes to high-severity defects.\n",
    "- Subtle, not structural, simplification:\n",
    "    - The near-zero Δdepth suggests most fixes are localized patches, not full refactors:\n",
    "    - Adding a null-check (if (x != null)) → +1 depth\n",
    "    - Removing a redundant branch → −1 depth\n",
    "    - Net effect cancels out on average.\n",
    "- Security fixes are unique:\n",
    "They are the only category showing:\n",
    "    - Negative mean Δdepth (−0.01)\n",
    "    - Reduction in % of methods ≥20 depth (9.01% → 8.78%)\n",
    "    - Suggests intentional simplification is part of secure coding practice (e.g., reducing branch conditions to eliminate att"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boa Client (Python 3.13)",
   "language": "python",
   "name": "boa-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
