{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issue Type Identification via Semantic Clustering\n",
    "\n",
    "This notebook implements a semantic clustering pipeline to identify common types of issues, bugs, and vulnerabilities in the dataset.\n",
    "\n",
    "**Methodology:**\n",
    "1. **Data Aggregation**: Group data by commit to create a rich textual representation (Commit Message + Changed Files + Changed Methods).\n",
    "2. **Embedding**: Use `all-MiniLM-L6-v2` (Sentence Transformers) to convert text to high-dimensional vectors.\n",
    "3. **Dimensionality Reduction**: Apply UMAP to reduce noise and computational complexity.\n",
    "4. **Clustering**: Use HDBSCAN (Density-based clustering) to find organic groups of similar commits.\n",
    "5. **Interpretability**: Extract top TF-IDF terms and exemplar commits to label each cluster.\n",
    "6. **Export**: Save statistics and plots for reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=pd.errors.ParserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "We load the largest available dataset (~1.1 GB csv) to get the most comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the largest CSV file\n",
    "data_dir = \"../assets/data-samples/\"\n",
    "if not os.path.exists(data_dir): \n",
    "    # Fallback if running from a different root\n",
    "    data_dir = \"assets/data-samples/\"\n",
    "\n",
    "csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {data_dir}\")\n",
    "\n",
    "csv_files_sorted = sorted(csv_files, key=lambda x: os.path.getsize(os.path.join(data_dir, x)), reverse=True)\n",
    "\n",
    "target_file = csv_files_sorted[0]\n",
    "file_path = os.path.join(data_dir, target_file)\n",
    "print(f\"Loading largest dataset: {target_file} ({os.path.getsize(file_path) / (1024**2):.2f} MB)\")\n",
    "\n",
    "# Read only relevant columns to save memory\n",
    "use_cols = ['project_name', 'commit_url', 'commit_message', 'file_path', 'method_name']\n",
    "df = pd.read_csv(file_path, usecols=use_cols, dtype='string')\n",
    "\n",
    "# Drop NA messages\n",
    "df = df.dropna(subset=['commit_message'])\n",
    "print(f\"Loaded {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Commit\n",
    "Each row in the raw CSV is a file/method change. We need a single text block per commit to cluster effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_commit(x):\n",
    "    # Unique files and methods\n",
    "    files = list(set(f for f in x['file_path'] if pd.notna(f)))\n",
    "    methods = list(set(m for m in x['method_name'] if pd.notna(m)))\n",
    "    \n",
    "    # Limit to top 5 files/methods to avoid super long texts\n",
    "    files_str = \", \".join(files[:5])\n",
    "    methods_str = \", \".join(methods[:5])\n",
    "    \n",
    "    msg = x['commit_message'].iloc[0]\n",
    "    \n",
    "    # Construct rich text\n",
    "    # Format: \"Log message | Files: A, B | Methods: X, Y\"\n",
    "    full_text = f\"{msg} | Files: {files_str} | Methods: {methods_str}\"\n",
    "    \n",
    "    return pd.Series({\n",
    "        'project_name': x['project_name'].iloc[0],\n",
    "        'commit_message': msg,\n",
    "        'full_text': full_text,\n",
    "        'n_files': len(files)\n",
    "    })\n",
    "\n",
    "# Grouping\n",
    "print(\"Aggregating by commit...\")\n",
    "commits_df = (\n",
    "    df.groupby('commit_url', group_keys=False)\n",
    "    .apply(aggregate_commit, include_groups=False)\n",
    "    .reset_index()\n",
    ")\n",
    "print(f\"Total unique commits: {len(commits_df)}\")\n",
    "\n",
    "commits_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Embedding Generation\n",
    "\n",
    "Using `all-MiniLM-L6-v2` for efficient, high-quality sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'\n",
    "print(f\"Loading model {model_name}...\")\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"Encoding texts (this may take a while)...\")\n",
    "embeddings = model.encode(commits_df['full_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "print(f\"Embedding shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction (UMAP)\n",
    "\n",
    "We reduce the 384-dimensional embeddings to 5 dimensions to help HDBSCAN find dense clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=50,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running UMAP reduction...\")\n",
    "umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "print(\"UMAP done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering (HDBSCAN)\n",
    "\n",
    "Density-based clustering to identify topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=20,      # Smallest size to consider a cluster\n",
    "    min_samples=5,            # How conservative the clustering is\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom', # Excess of Mass\n",
    "    prediction_data=True\n",
    ")\n",
    "\n",
    "print(\"Running HDBSCAN...\")\n",
    "cluster_labels = clusterer.fit_predict(umap_embeddings)\n",
    "commits_df['cluster'] = cluster_labels\n",
    "\n",
    "# -1 indicates noise\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"Found {n_clusters} clusters.\")\n",
    "print(f\"Noise points: {n_noise} ({n_noise/len(commits_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Analysis & Interpretation\n",
    "\n",
    "We calculate TF-IDF scores for the `full_text` within each cluster to find representative keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_tfidf_terms(df, cluster_col='cluster', text_col='full_text', top_n=6):\n",
    "    # Treat each cluster as a single document\n",
    "    valid_clusters = [c for c in df[cluster_col].unique() if c != -1]\n",
    "    cluster_docs = []\n",
    "    cluster_ids = []\n",
    "    \n",
    "    for c in valid_clusters:\n",
    "        c_texts = df[df[cluster_col] == c][text_col].tolist()\n",
    "        cluster_docs.append(\" \".join(c_texts))\n",
    "        cluster_ids.append(c)\n",
    "        \n",
    "    # Compute TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_df=0.6)\n",
    "    tfidf_matrix = vectorizer.fit_transform(cluster_docs)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    results = {}\n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        row = tfidf_matrix[i].toarray().flatten()\n",
    "        top_indices = row.argsort()[-top_n:][::-1]\n",
    "        top_terms = [feature_names[idx] for idx in top_indices]\n",
    "        results[cluster_id] = \", \".join(top_terms)\n",
    "        \n",
    "    return results\n",
    "\n",
    "print(\"Extracting top terms per cluster...\")\n",
    "top_terms_map = extract_top_tfidf_terms(commits_df)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster stats\n",
    "cluster_stats = commits_df['cluster'].value_counts().reset_index()\n",
    "cluster_stats.columns = ['Cluster ID', 'Size']\n",
    "\n",
    "# Filter out noise (-1)\n",
    "cluster_stats = cluster_stats[cluster_stats['Cluster ID'] != -1]\n",
    "\n",
    "# Map terms\n",
    "cluster_stats['Top Terms'] = cluster_stats['Cluster ID'].map(top_terms_map)\n",
    "\n",
    "# Sort by size\n",
    "cluster_stats = cluster_stats.sort_values('Size', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Add a placeholder for Manual Label\n",
    "cluster_stats['Manual Label'] = \"\"\n",
    "\n",
    "# Display top 20 clusters\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "display(cluster_stats.head(20))\n",
    "\n",
    "# Save to CSV\n",
    "output_dir = \"../analysis\"\n",
    "if not os.path.exists(output_dir):\n",
    "    output_dir = \"analysis\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Clusters (2D UMAP Projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to 2D for visualization\n",
    "umap_2d = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, random_state=42)\n",
    "proj_2d = umap_2d.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    proj_2d[:, 0], \n",
    "    proj_2d[:, 1], \n",
    "    c=cluster_labels, \n",
    "    cmap='Spectral', \n",
    "    s=5,\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('UMAP Projection of Commit Clusters', fontsize=16)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Boa Client (Python 3.13)",
   "language": "python",
   "name": "boa-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
